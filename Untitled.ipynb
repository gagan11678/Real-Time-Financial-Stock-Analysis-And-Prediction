{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a0640-7303-40a7-b21e-099a1e804e67",
   "metadata": {},
   "source": [
    "1. Code to Process and Store 60 Days Historical Merged Data to DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "309a80de-74ea-45b9-8c50-b26a990ef7e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:26:18.171397Z",
     "iopub.status.busy": "2025-04-11T16:26:18.171114Z",
     "iopub.status.idle": "2025-04-11T16:36:16.284709Z",
     "shell.execute_reply": "2025-04-11T16:36:16.283335Z",
     "shell.execute_reply.started": "2025-04-11T16:26:18.171375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 16:26:28.157270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2096/2096 [06:36<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 60-day data to DynamoDB...\n",
      "âœ… Historical 60-day data uploaded.\n",
      "Uploading merged CSV for dashboarding...\n",
      "âœ… CSV uploaded to S3 at s3://stock-market-data-uofc/dashboard-data/stock_60d_faded.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "from decimal import Decimal\n",
    "\n",
    "# --- AWS Configuration ---\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "table = dynamodb.Table(\"realtimedata\")\n",
    "\n",
    "# --- S3 Configuration ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"stock-market-data-uofc\"\n",
    "stock_key = \"historical-data/stock_60d_5m.csv\"\n",
    "news_key = \"news-data/news_history_1yr.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "stock_df = pd.read_csv(s3.get_object(Bucket=bucket, Key=stock_key)[\"Body\"])\n",
    "news_df = pd.read_csv(s3.get_object(Bucket=bucket, Key=news_key)[\"Body\"])\n",
    "\n",
    "# --- Preprocessing ---\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"], utc=True)\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.floor(\"min\")\n",
    "\n",
    "news_df[\"publishedDate\"] = pd.to_datetime(news_df[\"publishedDate\"], errors=\"coerce\")\n",
    "news_df[\"publishedDate\"] = news_df[\"publishedDate\"].dt.floor(\"min\")\n",
    "news_df.rename(columns={\"symbol\": \"Stock\"}, inplace=True)\n",
    "\n",
    "# --- Filter news in stock date range ---\n",
    "min_date = stock_df[\"Date\"].min().date()\n",
    "max_date = stock_df[\"Date\"].max().date()\n",
    "news_df = news_df[\n",
    "    (news_df[\"publishedDate\"].dt.date >= min_date) &\n",
    "    (news_df[\"publishedDate\"].dt.date <= max_date)\n",
    "]\n",
    "\n",
    "# --- Assign latest news to each stock row ---\n",
    "stock_df.sort_values([\"Stock\", \"Date\"], inplace=True)\n",
    "news_df.sort_values([\"Stock\", \"publishedDate\"], inplace=True)\n",
    "\n",
    "assigned_news = []\n",
    "for symbol, group in stock_df.groupby(\"Stock\"):\n",
    "    stock_times = group[\"Date\"].values\n",
    "    news_times = news_df[news_df[\"Stock\"] == symbol][[\"publishedDate\", \"text\"]].values\n",
    "    latest_news = None\n",
    "    news_index = 0\n",
    "    for stock_time in stock_times:\n",
    "        while news_index < len(news_times) and news_times[news_index][0] <= stock_time:\n",
    "            latest_news = news_times[news_index][1]\n",
    "            news_index += 1\n",
    "        assigned_news.append(latest_news)\n",
    "\n",
    "stock_df[\"latest_news\"] = assigned_news\n",
    "\n",
    "# --- Sentiment Analysis (FinBERT) ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            scores = probs[0].tolist()\n",
    "            return -1 * scores[0] + 0 * scores[1] + 1 * scores[2]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "unique_news = stock_df[\"latest_news\"].dropna().unique()\n",
    "news_sentiment_map = {news: get_sentiment(news) for news in tqdm(unique_news)}\n",
    "\n",
    "# --- Sentiment Fading ---\n",
    "faded_scores = []\n",
    "fade_factor = 0.9\n",
    "prev_news = None\n",
    "prev_score = 0.0\n",
    "repeat_count = 0\n",
    "\n",
    "for news in stock_df[\"latest_news\"]:\n",
    "    if news != prev_news:\n",
    "        sentiment = news_sentiment_map.get(news, 0.0)\n",
    "        repeat_count = 0\n",
    "    else:\n",
    "        repeat_count += 1\n",
    "        sentiment = prev_score * (fade_factor ** repeat_count)\n",
    "    faded_scores.append(round(sentiment, 4))\n",
    "    prev_news = news\n",
    "    prev_score = news_sentiment_map.get(news, 0.0)\n",
    "\n",
    "stock_df[\"faded_sentiment_score\"] = faded_scores\n",
    "\n",
    "# --- Upload to DynamoDB ---\n",
    "print(\"Uploading 60-day data to DynamoDB...\")\n",
    "for _, row in stock_df.iterrows():\n",
    "    try:\n",
    "        table.put_item(Item={\n",
    "            \"Stock\": row[\"Stock\"],\n",
    "            \"Timestamp\": row[\"Date\"].strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "            \"Open\": Decimal(str(row[\"Open\"])),\n",
    "            \"High\": Decimal(str(row[\"High\"])),\n",
    "            \"Low\": Decimal(str(row[\"Low\"])),\n",
    "            \"Close\": Decimal(str(row[\"Close\"])),\n",
    "            \"Volume\": int(row[\"Volume\"]),\n",
    "            \"faded_sentiment_score\": Decimal(str(row[\"faded_sentiment_score\"]))\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting row: {e}\")\n",
    "\n",
    "print(\"âœ… Historical 60-day data uploaded.\")\n",
    "\n",
    "\n",
    "# --- Save Final Dataset to S3 for Dashboard ---\n",
    "print(\"Uploading merged CSV for dashboarding...\")\n",
    "\n",
    "csv_buffer = stock_df.to_csv(index=False)\n",
    "dashboard_key = \"dashboard-data/stock_60d_faded.csv\"\n",
    "\n",
    "s3.put_object(Bucket=bucket, Key=dashboard_key, Body=csv_buffer)\n",
    "\n",
    "print(f\"âœ… CSV uploaded to S3 at s3://{bucket}/{dashboard_key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c97b8-8cc7-4494-9923-1b43a6e15a9a",
   "metadata": {},
   "source": [
    "Updated Real-Time Ingestion Script (every 5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "860b7328-da9b-4a58-a543-4a23e69b4551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:52:16.539274Z",
     "iopub.status.busy": "2025-04-11T16:52:16.537601Z",
     "iopub.status.idle": "2025-04-11T16:52:26.336685Z",
     "shell.execute_reply": "2025-04-11T16:52:26.335648Z",
     "shell.execute_reply.started": "2025-04-11T16:52:16.539238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Ingestion + Prediction job started\n",
      "âœ… Data ingested & predictions made\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# âœ… Real-Time Fading Sentiment Integration\n",
    "# =====================================\n",
    "\n",
    "# --- STEP 1: Create sentiment_tracker table ---\n",
    "# In your DynamoDB console or via CLI, create:\n",
    "# Table name: sentiment_tracker\n",
    "# Partition key: Stock (String)\n",
    "\n",
    "# Each entry will store:\n",
    "# - Stock\n",
    "# - LastNews (string)\n",
    "# - LastScore (float)\n",
    "# - RepeatCount (int)\n",
    "\n",
    "# --- STEP 2: Modify Real-Time Ingestion Script ---\n",
    "# Final Real-Time Ingestion + Sentiment Fading + Prediction Script\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import requests\n",
    "import torch\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from decimal import Decimal\n",
    "import pytz\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# AWS Setup\n",
    "session = boto3.Session()\n",
    "dynamodb = session.resource(\"dynamodb\")\n",
    "data_table = dynamodb.Table(\"realtimedata\")\n",
    "tracker_table = dynamodb.Table(\"news_tracker\")\n",
    "sentiment_table = dynamodb.Table(\"sentiment_tracker\")\n",
    "pred_table = dynamodb.Table(\"realtime_predictions\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Constants\n",
    "STOCKS = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "API_KEY = \"hDdU807PlusyraTBnNgkkm2gFuPtkZ9F\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "MODEL_PREFIX = \"models/sarimax\"\n",
    "LOCAL_MODEL_PATH = \"/tmp\"\n",
    "\n",
    "# FinBERT Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Utility Functions\n",
    "def is_market_open():\n",
    "    est = pytz.timezone('US/Eastern')\n",
    "    now = datetime.now(est)\n",
    "    return now.weekday() < 5 and datetime.strptime(\"09:30\", \"%H:%M\").time() <= now.time() <= datetime.strptime(\"16:00\", \"%H:%M\").time()\n",
    "\n",
    "def fetch_stock_data():\n",
    "    if not is_market_open():\n",
    "        return pd.DataFrame()\n",
    "    all_data = []\n",
    "    for stock in STOCKS:\n",
    "        df = yf.Ticker(stock).history(period=\"1d\", interval=\"5m\")\n",
    "        if df.empty: continue\n",
    "        df.reset_index(inplace=True)\n",
    "        df[\"Stock\"] = stock\n",
    "        df.rename(columns={\"Datetime\": \"Date\"}, inplace=True)\n",
    "        all_data.append(df[[\"Stock\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "def fetch_news():\n",
    "    all_news = []\n",
    "    for stock in STOCKS:\n",
    "        try:\n",
    "            item = tracker_table.get_item(Key={\"Stock\": stock, \"LastPublished\": \"latest\"}).get(\"Item\")\n",
    "            last_date = pd.to_datetime(item[\"LastDate\"])\n",
    "            last_title = item[\"LastTitle\"]\n",
    "        except:\n",
    "            last_date = datetime.utcnow() - timedelta(days=1)\n",
    "            last_title = \"\"\n",
    "        res = requests.get(\"https://financialmodelingprep.com/api/v3/stock_news\", params={\n",
    "            \"tickers\": stock,\n",
    "            \"from\": last_date.strftime('%Y-%m-%d'),\n",
    "            \"to\": datetime.utcnow().strftime('%Y-%m-%d'),\n",
    "            \"limit\": 50,\n",
    "            \"apikey\": API_KEY\n",
    "        })\n",
    "        if res.status_code == 200:\n",
    "            articles = res.json()\n",
    "            new_articles = [a for a in articles if pd.to_datetime(a[\"publishedDate\"]) > last_date or a[\"title\"] != last_title]\n",
    "            if new_articles:\n",
    "                latest = pd.to_datetime(new_articles[0][\"publishedDate\"])\n",
    "                tracker_table.put_item(Item={\n",
    "                    \"Stock\": stock,\n",
    "                    \"LastPublished\": \"latest\",\n",
    "                    \"LastDate\": latest.isoformat(),\n",
    "                    \"LastTitle\": new_articles[0][\"title\"]\n",
    "                })\n",
    "                all_news.extend(new_articles)\n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            return -1 * probs[0][0].item() + probs[0][2].item()\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_faded_score(stock, latest_news, score, fade_factor=0.9):\n",
    "    try:\n",
    "        entry = sentiment_table.get_item(Key={\"Stock\": stock}).get(\"Item\")\n",
    "        prev_news = entry.get(\"LastNews\")\n",
    "        prev_score = float(entry.get(\"LastScore\"))\n",
    "        repeat_count = int(entry.get(\"RepeatCount\"))\n",
    "        if latest_news == prev_news:\n",
    "            repeat_count += 1\n",
    "            faded_score = prev_score * (fade_factor ** repeat_count)\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "            faded_score = score\n",
    "    except:\n",
    "        faded_score = score\n",
    "        repeat_count = 0\n",
    "    sentiment_table.put_item(Item={\n",
    "        \"Stock\": stock,\n",
    "        \"LastNews\": latest_news,\n",
    "        \"LastScore\": Decimal(str(round(score, 4))),\n",
    "        \"RepeatCount\": repeat_count\n",
    "    })\n",
    "    return round(faded_score, 4)\n",
    "\n",
    "def add_features(df):\n",
    "    df[\"EMA_5\"] = df[\"Close\"].ewm(span=5).mean()\n",
    "    df[\"EMA_12\"] = df[\"Close\"].ewm(span=12).mean()\n",
    "    df[\"Volatility_30min\"] = df[\"Close\"].rolling(window=6).std()\n",
    "    df[\"Price_Change_Pct\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Lag_Close\"] = df[\"Close\"].shift(1)\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    return df\n",
    "\n",
    "def load_model_from_s3(stock):\n",
    "    local_path = os.path.join(LOCAL_MODEL_PATH, f\"{stock}.pkl\")\n",
    "    s3.download_file(S3_BUCKET, f\"{MODEL_PREFIX}/{stock}.pkl\", local_path)\n",
    "    return joblib.load(local_path)\n",
    "\n",
    "def make_prediction_row(stock, latest_row, model_obj):\n",
    "    features = model_obj[\"features\"]\n",
    "    scaler = model_obj[\"scaler\"]\n",
    "    model = model_obj[\"model\"]\n",
    "\n",
    "    X = scaler.transform(latest_row[features])\n",
    "    pred_15 = model.forecast(steps=3, exog=X.repeat(3, axis=0)).mean()\n",
    "    pred_30 = model.forecast(steps=6, exog=X.repeat(6, axis=0)).mean()\n",
    "    pred_60 = model.forecast(steps=12, exog=X.repeat(12, axis=0)).mean()\n",
    "\n",
    "    return {\n",
    "        \"Stock\": stock,\n",
    "        \"Timestamp\": latest_row[\"Date\"].iloc[0].isoformat(),\n",
    "        \"Current_Close\": round(float(latest_row[\"Close\"]), 4),\n",
    "        \"Pred_15min\": round(float(pred_15), 4),\n",
    "        \"Pred_30min\": round(float(pred_30), 4),\n",
    "        \"Pred_60min\": round(float(pred_60), 4)\n",
    "    }\n",
    "\n",
    "def process_all(stock_df, news_df):\n",
    "    news_df = news_df.sort_values(\"publishedDate\")\n",
    "    sentiment_scores = {}\n",
    "    for stock in STOCKS:\n",
    "        latest_news = news_df[news_df[\"symbol\"] == stock].sort_values(\"publishedDate\").tail(1)\n",
    "        if latest_news.empty:\n",
    "            sentiment_scores[stock] = Decimal(\"0.0\")\n",
    "            continue\n",
    "        text = latest_news.iloc[0][\"text\"]\n",
    "        raw_score = get_sentiment(text)\n",
    "        faded = get_faded_score(stock, text, raw_score)\n",
    "        sentiment_scores[stock] = Decimal(str(faded))\n",
    "\n",
    "    prediction_rows = []\n",
    "    for stock in STOCKS:\n",
    "        sdf = stock_df[stock_df[\"Stock\"] == stock].copy()\n",
    "        if sdf.empty: continue\n",
    "        score = sentiment_scores.get(stock, Decimal(\"0.0\"))\n",
    "        sdf[\"faded_sentiment_score\"] = float(score)\n",
    "        sdf = add_features(sdf)\n",
    "        sdf.dropna(inplace=True)\n",
    "        if sdf.empty: continue\n",
    "\n",
    "        # Use last row for Dynamo + Prediction\n",
    "        latest = sdf.tail(1).copy()\n",
    "        item = {\n",
    "            \"Stock\": stock,\n",
    "            \"Timestamp\": latest[\"Date\"].iloc[0].isoformat(),\n",
    "            \"Open\": Decimal(str(latest[\"Open\"].iloc[0])),\n",
    "            \"High\": Decimal(str(latest[\"High\"].iloc[0])),\n",
    "            \"Low\": Decimal(str(latest[\"Low\"].iloc[0])),\n",
    "            \"Close\": Decimal(str(latest[\"Close\"].iloc[0])),\n",
    "            \"Volume\": int(latest[\"Volume\"].iloc[0]),\n",
    "            \"faded_sentiment_score\": Decimal(str(latest[\"faded_sentiment_score\"].iloc[0]))\n",
    "        }\n",
    "        data_table.put_item(Item=item)\n",
    "\n",
    "        try:\n",
    "            model_obj = load_model_from_s3(stock)\n",
    "            pred_row = make_prediction_row(stock, latest, model_obj)\n",
    "            pred_table.put_item(Item={k: (Decimal(str(v)) if isinstance(v, float) else v) for k, v in pred_row.items()})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Ingestion + Prediction job started\")\n",
    "    stock_data = fetch_stock_data()\n",
    "    if not stock_data.empty:\n",
    "        news_data = fetch_news()\n",
    "        process_all(stock_data, news_data)\n",
    "        print(\"âœ… Data ingested & predictions made\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Market closed or no stock data\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a7d09-61e2-4510-a417-9984af1829cd",
   "metadata": {},
   "source": [
    "final model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74f3623-6366-4374-9418-d24f039887e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T19:19:19.287079Z",
     "iopub.status.busy": "2025-04-11T19:19:19.286738Z",
     "iopub.status.idle": "2025-04-11T19:19:20.822205Z",
     "shell.execute_reply": "2025-04-11T19:19:20.821296Z",
     "shell.execute_reply.started": "2025-04-11T19:19:19.287057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading data from DynamoDB...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains when parsing with format \"%Y-%m-%dT%H:%M:%S\": \"-04:00\", at position 3307. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš€ Loading data from DynamoDB...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_dynamodb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m     stocks \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStock\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mload_data_from_dynamodb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(items)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert to correct types\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaded_sentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     41\u001b[0m     df[col] \u001b[38;5;241m=\u001b[39m df[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:587\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains when parsing with format \"%Y-%m-%dT%H:%M:%S\": \"-04:00\", at position 3307. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from decimal import Decimal\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config ---\n",
    "LOCAL_MODEL_DIR = \"/tmp/trained_models\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_PREFIX = \"models/sarimax\"\n",
    "DYNAMO_TABLE = \"realtimedata\"\n",
    "\n",
    "# Create local model directory\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load Data from DynamoDB ---\n",
    "def load_data_from_dynamodb():\n",
    "    dynamodb = boto3.resource(\"dynamodb\")\n",
    "    table = dynamodb.Table(DYNAMO_TABLE)\n",
    "\n",
    "    response = table.scan()\n",
    "    items = response[\"Items\"]\n",
    "\n",
    "    # Keep paginating if needed\n",
    "    while \"LastEvaluatedKey\" in response:\n",
    "        response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n",
    "        items.extend(response[\"Items\"])\n",
    "\n",
    "    df = pd.DataFrame(items)\n",
    "\n",
    "    # Convert to correct types\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"faded_sentiment_score\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    df[\"Volume\"] = df[\"Volume\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "def add_intraday_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"EMA_5\"] = df[\"Close\"].ewm(span=5, adjust=False).mean()\n",
    "    df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"Volatility_30min\"] = df[\"Close\"].rolling(window=6).std()\n",
    "    df[\"Price_Change_Pct\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Lag_Close\"] = df[\"Close\"].shift(1)\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    return df\n",
    "\n",
    "# --- Train & Upload Model ---\n",
    "def train_and_save_model(df, stock, s3_client):\n",
    "    stock_df = df[df[\"Stock\"] == stock].copy()\n",
    "    stock_df = add_intraday_features(stock_df)\n",
    "    stock_df.set_index(\"Date\", inplace=True)\n",
    "    stock_df.sort_index(inplace=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Volume\",\n",
    "        \"faded_sentiment_score\",\n",
    "        \"EMA_5\",\n",
    "        \"EMA_12\",\n",
    "        \"Volatility_30min\",\n",
    "        \"Price_Change_Pct\",\n",
    "        \"Lag_Close\",\n",
    "        \"Weekday\"\n",
    "    ]\n",
    "\n",
    "    ts_df = stock_df[[\"Close\"] + feature_cols].dropna()\n",
    "    if len(ts_df) < 100:\n",
    "        return\n",
    "\n",
    "    endog = ts_df[\"Close\"]\n",
    "    exog = ts_df[feature_cols]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    exog_scaled = pd.DataFrame(scaler.fit_transform(exog), columns=feature_cols, index=exog.index)\n",
    "\n",
    "    try:\n",
    "        arima_model = auto_arima(\n",
    "            endog, exogenous=exog_scaled,\n",
    "            seasonal=False, stepwise=True,\n",
    "            suppress_warnings=True, max_order=6, maxiter=30\n",
    "        )\n",
    "        order = arima_model.order\n",
    "    except:\n",
    "        order = (2, 1, 2)\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(endog, exog=exog_scaled, order=order, enforce_stationarity=False)\n",
    "        result = model.fit(disp=False, maxiter=50)\n",
    "\n",
    "        local_path = os.path.join(LOCAL_MODEL_DIR, f\"{stock}.pkl\")\n",
    "        joblib.dump({\n",
    "            \"model\": result,\n",
    "            \"scaler\": scaler,\n",
    "            \"features\": feature_cols,\n",
    "            \"order\": order\n",
    "        }, local_path)\n",
    "\n",
    "        s3_key = f\"{S3_PREFIX}/{stock}.pkl\"\n",
    "        s3_client.upload_file(local_path, S3_BUCKET, s3_key)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Loading data from DynamoDB...\")\n",
    "    df = load_data_from_dynamodb()\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    stocks = df[\"Stock\"].unique()\n",
    "    for stock in stocks:\n",
    "        train_and_save_model(df, stock, s3)\n",
    "    print(\"âœ… Model training & upload complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a496387-6e04-436b-a8d3-d15691eaac6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T19:18:58.703774Z",
     "iopub.status.busy": "2025-04-11T19:18:58.703461Z",
     "iopub.status.idle": "2025-04-11T19:19:01.998112Z",
     "shell.execute_reply": "2025-04-11T19:19:01.997402Z",
     "shell.execute_reply.started": "2025-04-11T19:18:58.703747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\n",
      "  Downloading pmdarima-2.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (1.4.2)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima)\n",
      "  Downloading Cython-3.0.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (1.26.19)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (75.3.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.12/site-packages (from pmdarima) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=0.19->pmdarima) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.12/site-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.16.0)\n",
      "Downloading pmdarima-2.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Cython-3.0.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m141.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Cython, pmdarima\n",
      "Successfully installed Cython-3.0.12 pmdarima-2.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efecbcff-d44d-47ff-8dc1-8ddcde0b4eaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T07:03:54.011841Z",
     "iopub.status.busy": "2025-04-10T07:03:54.011198Z",
     "iopub.status.idle": "2025-04-10T07:03:55.787425Z",
     "shell.execute_reply": "2025-04-10T07:03:55.786666Z",
     "shell.execute_reply.started": "2025-04-10T07:03:54.011812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.55-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.4.6)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.17.9-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Using cached yfinance-0.2.55-py2.py3-none-any.whl (109 kB)\n",
      "Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: peewee, multitasking, yfinance\n",
      "Successfully installed multitasking-0.0.11 peewee-3.17.9 yfinance-0.2.55\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448627af-0cd7-4026-a4ae-fee5d2412cf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T07:25:25.929966Z",
     "iopub.status.busy": "2025-04-10T07:25:25.929446Z",
     "iopub.status.idle": "2025-04-10T07:27:59.995530Z",
     "shell.execute_reply": "2025-04-10T07:27:59.994557Z",
     "shell.execute_reply.started": "2025-04-10T07:25:25.929943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading data from DynamoDB...\n",
      "âœ… Model training & upload complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from decimal import Decimal\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config ---\n",
    "LOCAL_MODEL_DIR = \"/tmp/trained_models\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_PREFIX = \"models/sarimax\"\n",
    "DYNAMO_TABLE = \"realtimedata\"\n",
    "\n",
    "# Create local model directory\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load Data from DynamoDB ---\n",
    "def load_data_from_dynamodb():\n",
    "    dynamodb = boto3.resource(\"dynamodb\")\n",
    "    table = dynamodb.Table(DYNAMO_TABLE)\n",
    "\n",
    "    response = table.scan()\n",
    "    items = response[\"Items\"]\n",
    "\n",
    "    # Keep paginating if needed\n",
    "    while \"LastEvaluatedKey\" in response:\n",
    "        response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n",
    "        items.extend(response[\"Items\"])\n",
    "\n",
    "    df = pd.DataFrame(items)\n",
    "\n",
    "    # âœ… Fixed datetime parsing with timezone and error handling\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\", utc=True)\n",
    "    df = df[df[\"Date\"].notna()]  # Drop rows with invalid datetime\n",
    "\n",
    "    # Convert to correct types\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"faded_sentiment_score\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    df[\"Volume\"] = df[\"Volume\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "def add_intraday_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"EMA_5\"] = df[\"Close\"].ewm(span=5, adjust=False).mean()\n",
    "    df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"Volatility_30min\"] = df[\"Close\"].rolling(window=6).std()\n",
    "    df[\"Price_Change_Pct\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Lag_Close\"] = df[\"Close\"].shift(1)\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    return df\n",
    "\n",
    "# --- Train & Upload Model ---\n",
    "def train_and_save_model(df, stock, s3_client):\n",
    "    stock_df = df[df[\"Stock\"] == stock].copy()\n",
    "    stock_df = add_intraday_features(stock_df)\n",
    "    stock_df.set_index(\"Date\", inplace=True)\n",
    "    stock_df.sort_index(inplace=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Volume\",\n",
    "        \"faded_sentiment_score\",\n",
    "        \"EMA_5\",\n",
    "        \"EMA_12\",\n",
    "        \"Volatility_30min\",\n",
    "        \"Price_Change_Pct\",\n",
    "        \"Lag_Close\",\n",
    "        \"Weekday\"\n",
    "    ]\n",
    "\n",
    "    ts_df = stock_df[[\"Close\"] + feature_cols].dropna()\n",
    "    if len(ts_df) < 100:\n",
    "        return\n",
    "\n",
    "    endog = ts_df[\"Close\"]\n",
    "    exog = ts_df[feature_cols]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    exog_scaled = pd.DataFrame(scaler.fit_transform(exog), columns=feature_cols, index=exog.index)\n",
    "\n",
    "    try:\n",
    "        arima_model = auto_arima(\n",
    "            endog, exogenous=exog_scaled,\n",
    "            seasonal=False, stepwise=True,\n",
    "            suppress_warnings=True, max_order=6, maxiter=30\n",
    "        )\n",
    "        order = arima_model.order\n",
    "    except:\n",
    "        order = (2, 1, 2)\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(endog, exog=exog_scaled, order=order, enforce_stationarity=False)\n",
    "        result = model.fit(disp=False, maxiter=50)\n",
    "\n",
    "        local_path = os.path.join(LOCAL_MODEL_DIR, f\"{stock}.pkl\")\n",
    "        joblib.dump({\n",
    "            \"model\": result,\n",
    "            \"scaler\": scaler,\n",
    "            \"features\": feature_cols,\n",
    "            \"order\": order\n",
    "        }, local_path)\n",
    "\n",
    "        s3_key = f\"{S3_PREFIX}/{stock}.pkl\"\n",
    "        s3_client.upload_file(local_path, S3_BUCKET, s3_key)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Loading data from DynamoDB...\")\n",
    "    df = load_data_from_dynamodb()\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    stocks = df[\"Stock\"].unique()\n",
    "    for stock in stocks:\n",
    "        train_and_save_model(df, stock, s3)\n",
    "    print(\"âœ… Model training & upload complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a5e7160-0943-49ed-bff3-df89185f7b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:54:49.050124Z",
     "iopub.status.busy": "2025-04-11T16:54:49.049718Z",
     "iopub.status.idle": "2025-04-11T16:54:50.190868Z",
     "shell.execute_reply": "2025-04-11T16:54:50.189173Z",
     "shell.execute_reply.started": "2025-04-11T16:54:49.050096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Exporting realtimedata...\n",
      "âœ… Uploaded dashboard-data/realtimedata.csv to S3.\n",
      "ðŸ“¦ Exporting realtime_predictions...\n",
      "âœ… Uploaded dashboard-data/realtime_predictions.csv to S3.\n",
      "ðŸ“¦ Exporting news_tracker...\n",
      "âœ… Uploaded dashboard-data/news_tracker.csv to S3.\n",
      "ðŸ“¦ Exporting sentiment_tracker...\n",
      "âœ… Uploaded dashboard-data/sentiment_tracker.csv to S3.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# AWS Config\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# S3 Bucket Info\n",
    "bucket_name = \"stock-market-data-uofc\"\n",
    "s3_prefix = \"dashboard-data/\"  # Optional subfolder\n",
    "\n",
    "# List of tables to export\n",
    "tables_to_export = [\n",
    "    \"realtimedata\",\n",
    "    \"realtime_predictions\",\n",
    "    \"news_tracker\",\n",
    "    \"sentiment_tracker\"\n",
    "]\n",
    "\n",
    "# Function to export table\n",
    "def export_table_to_csv(table_name):\n",
    "    print(f\"ðŸ“¦ Exporting {table_name}...\")\n",
    "\n",
    "    table = dynamodb.Table(table_name)\n",
    "    response = table.scan()\n",
    "    data = response['Items']\n",
    "\n",
    "    if not data:\n",
    "        print(f\"âš ï¸ No data in {table_name}\")\n",
    "        return\n",
    "\n",
    "    keys = list(data[0].keys())\n",
    "    file_name = f\"/tmp/{table_name}today.csv\"\n",
    "\n",
    "    # Write CSV locally\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_key = f\"{s3_prefix}{table_name}.csv\"\n",
    "    s3.upload_file(file_name, bucket_name, s3_key)\n",
    "    print(f\"âœ… Uploaded {s3_key} to S3.\")\n",
    "\n",
    "# Loop through all tables\n",
    "for table in tables_to_export:\n",
    "    export_table_to_csv(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce0b55-da70-4862-91ac-c6f3d42c4a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580e2a96-81f0-4905-9559-950a1c9d072e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T15:54:21.272482Z",
     "iopub.status.busy": "2025-04-11T15:54:21.271298Z",
     "iopub.status.idle": "2025-04-11T15:54:22.682618Z",
     "shell.execute_reply": "2025-04-11T15:54:22.678547Z",
     "shell.execute_reply.started": "2025-04-11T15:54:21.272420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing data found. Fetching fresh...\n",
      "Fetching AAPL from 2024-04-11...\n",
      "Fetching MSFT from 2024-04-11...\n",
      "Fetching AMZN from 2024-04-11...\n",
      "Fetching GOOGL from 2024-04-11...\n",
      "Fetching META from 2024-04-11...\n",
      "Fetching NVDA from 2024-04-11...\n",
      "Fetching TSLA from 2024-04-11...\n",
      "Uploaded to S3: historical-data/stock_history_1y.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS S3 Configuration\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"historical-data/stock_history_1y.csv\"\n",
    "#S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "#S3_KEY = \"historical-stock-data/stock_1y_1d.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# List of stocks to fetch\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "# Fetch stock data from a given start date\n",
    "def fetch_stock_data(start_date):\n",
    "    all_stock_data = []\n",
    "    for stock in stocks:\n",
    "        print(f\"Fetching {stock} from {start_date}...\")\n",
    "        ticker = yf.Ticker(stock)\n",
    "        history = ticker.history(start=start_date, interval=\"1d\")\n",
    "        history.reset_index(inplace=True)\n",
    "        history[\"Stock\"] = stock\n",
    "        history = history[[\"Stock\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        all_stock_data.append(history)\n",
    "    return pd.concat(all_stock_data, ignore_index=True)\n",
    "\n",
    "# Load data from S3\n",
    "def load_existing_data():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_data = pd.read_csv(response['Body'], parse_dates=[\"Date\"])\n",
    "        print(f\"Loaded existing data: {len(existing_data)} rows\")\n",
    "        return existing_data\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing data found. Fetching fresh...\")\n",
    "        return None\n",
    "\n",
    "# Save DataFrame to S3\n",
    "def upload_to_s3(df):\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer)\n",
    "    print(f\"Uploaded to S3: {S3_KEY}\")\n",
    "\n",
    "# Main logic\n",
    "existing_df = load_existing_data()\n",
    "\n",
    "if existing_df is not None:\n",
    "    last_date = existing_df[\"Date\"].max().date()\n",
    "    next_date = last_date + pd.Timedelta(days=1)\n",
    "    new_data = fetch_stock_data(start_date=next_date)\n",
    "    combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"Stock\", \"Date\"], inplace=True)\n",
    "else:\n",
    "    start_date = (datetime.now() - pd.Timedelta(days=365)).date()\n",
    "    combined_df = fetch_stock_data(start_date=start_date)\n",
    "\n",
    "upload_to_s3(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3831c56a-3e75-40ad-9e6e-b9523618c6f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:03:48.234158Z",
     "iopub.status.busy": "2025-04-11T16:03:48.233760Z",
     "iopub.status.idle": "2025-04-11T16:03:51.686811Z",
     "shell.execute_reply": "2025-04-11T16:03:51.686013Z",
     "shell.execute_reply.started": "2025-04-11T16:03:48.234131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing data found in S3.\n",
      "First-time fetch for last 60 days (5m)...\n",
      "Fetching AAPL from 2025-02-10 16:03:48.371494...\n",
      "Fetching MSFT from 2025-02-10 16:03:48.371494...\n",
      "Fetching AMZN from 2025-02-10 16:03:48.371494...\n",
      "Fetching GOOGL from 2025-02-10 16:03:48.371494...\n",
      "Fetching META from 2025-02-10 16:03:48.371494...\n",
      "Fetching NVDA from 2025-02-10 16:03:48.371494...\n",
      "Fetching TSLA from 2025-02-10 16:03:48.371494...\n",
      "Uploaded updated dataset to S3: historical-data/stock_60d_5m.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import timezone\n",
    "import pytz\n",
    "start_time = datetime.now(pytz.timezone(\"America/New_York\")).replace(tzinfo=None) - timedelta(days=60)\n",
    "\n",
    "\n",
    "# AWS S3 Configuration\n",
    "#S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "#bucket = \"stock-market-data-uofc\"\n",
    "#stock_key = \"historical-data/stock_60d_5m.csv\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"historical-data/stock_60d_5m.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Stocks to fetch\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "# Function to fetch 5-minute interval data for the last 60 days\n",
    "def fetch_stock_data(start_datetime):\n",
    "    all_data = []\n",
    "    for stock in stocks:\n",
    "        print(f\"Fetching {stock} from {start_datetime}...\")\n",
    "        ticker = yf.Ticker(stock)\n",
    "        history = ticker.history(period=\"60d\", interval=\"5m\")\n",
    "        history.reset_index(inplace=True)\n",
    "        history[\"Stock\"] = stock\n",
    "        history = history.rename(columns={\"Datetime\": \"Date\"})\n",
    "        history[\"Date\"] = history[\"Date\"].dt.tz_localize(None)  # Remove timezone info\n",
    "        history = history[[\"Stock\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        history = history[history[\"Date\"] > start_datetime]\n",
    "        all_data.append(history)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Load existing data from S3\n",
    "def load_existing_data():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_df = pd.read_csv(response['Body'], parse_dates=[\"Date\"])\n",
    "        print(f\"Loaded existing data: {len(existing_df)} rows\")\n",
    "        return existing_df\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing data found in S3.\")\n",
    "        return None\n",
    "\n",
    "# Upload DataFrame to S3\n",
    "def upload_to_s3(df):\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer)\n",
    "    print(f\"Uploaded updated dataset to S3: {S3_KEY}\")\n",
    "\n",
    "# Main logic\n",
    "existing_df = load_existing_data()\n",
    "\n",
    "if existing_df is not None:\n",
    "    last_time = existing_df[\"Date\"].max()\n",
    "    new_data = fetch_stock_data(start_datetime=last_time)\n",
    "    combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"Stock\", \"Date\"], inplace=True)\n",
    "else:\n",
    "    print(\"First-time fetch for last 60 days (5m)...\")\n",
    "    start_time = datetime.now() - timedelta(days=60)\n",
    "    combined_df = fetch_stock_data(start_datetime=start_time)\n",
    "\n",
    "upload_to_s3(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5331ea9-169f-4b21-a657-9924c9b13bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:06:01.969360Z",
     "iopub.status.busy": "2025-04-11T16:06:01.968876Z",
     "iopub.status.idle": "2025-04-11T16:13:05.547679Z",
     "shell.execute_reply": "2025-04-11T16:13:05.545581Z",
     "shell.execute_reply.started": "2025-04-11T16:06:01.969332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing news file found in S3.\n",
      "First-time fetch from: 2024-04-11\n",
      "Fetching news from 2024-04-11 to 2025-04-11\n",
      "Fetching page 1\n",
      "Fetching page 2\n",
      "Fetching page 3\n",
      "Fetching page 4\n",
      "Fetching page 5\n",
      "Fetching page 6\n",
      "Fetching page 7\n",
      "Fetching page 8\n",
      "Fetching page 9\n",
      "Fetching page 10\n",
      "Fetching page 11\n",
      "Fetching page 12\n",
      "Fetching page 13\n",
      "Fetching page 14\n",
      "Fetching page 15\n",
      "Fetching page 16\n",
      "Fetching page 17\n",
      "Fetching page 18\n",
      "Fetching page 19\n",
      "Fetching page 20\n",
      "Fetching page 21\n",
      "Fetching page 22\n",
      "Fetching page 23\n",
      "Fetching page 24\n",
      "Fetching page 25\n",
      "Fetching page 26\n",
      "Fetching page 27\n",
      "Fetching page 28\n",
      "Fetching page 29\n",
      "Fetching page 30\n",
      "Fetching page 31\n",
      "Fetching page 32\n",
      "Fetching page 33\n",
      "Fetching page 34\n",
      "Fetching page 35\n",
      "Fetching page 36\n",
      "Fetching page 37\n",
      "Fetching page 38\n",
      "Fetching page 39\n",
      "Fetching page 40\n",
      "Fetching page 41\n",
      "Fetching page 42\n",
      "Fetching page 43\n",
      "Fetching page 44\n",
      "Fetching page 45\n",
      "Fetching page 46\n",
      "Fetching page 47\n",
      "Fetching page 48\n",
      "Fetching page 49\n",
      "Fetching page 50\n",
      "Fetching page 51\n",
      "Fetching page 52\n",
      "Fetching page 53\n",
      "Fetching page 54\n",
      "Fetching page 55\n",
      "Fetching page 56\n",
      "Fetching page 57\n",
      "Fetching page 58\n",
      "Fetching page 59\n",
      "Fetching page 60\n",
      "Fetching page 61\n",
      "Fetching page 62\n",
      "Fetching page 63\n",
      "Fetching page 64\n",
      "Fetching page 65\n",
      "Fetching page 66\n",
      "Fetching page 67\n",
      "Fetching page 68\n",
      "Fetching page 69\n",
      "Fetching page 70\n",
      "Fetching page 71\n",
      "Fetching page 72\n",
      "Fetching page 73\n",
      "Fetching page 74\n",
      "Fetching page 75\n",
      "Fetching page 76\n",
      "Fetching page 77\n",
      "Fetching page 78\n",
      "Fetching page 79\n",
      "Fetching page 80\n",
      "Fetching page 81\n",
      "Fetching page 82\n",
      "Fetching page 83\n",
      "Fetching page 84\n",
      "Fetching page 85\n",
      "Fetching page 86\n",
      "Fetching page 87\n",
      "Fetching page 88\n",
      "Fetching page 89\n",
      "Fetching page 90\n",
      "Fetching page 91\n",
      "Fetching page 92\n",
      "Fetching page 93\n",
      "Fetching page 94\n",
      "Fetching page 95\n",
      "Fetching page 96\n",
      "Fetching page 97\n",
      "Fetching page 98\n",
      "Fetching page 99\n",
      "Fetching page 100\n",
      "Fetching page 101\n",
      "Fetching page 102\n",
      "Fetching page 103\n",
      "Fetching page 104\n",
      "Fetching page 105\n",
      "Fetching page 106\n",
      "Fetching page 107\n",
      "Fetching page 108\n",
      "Fetching page 109\n",
      "Fetching page 110\n",
      "Fetching page 111\n",
      "Fetching page 112\n",
      "Fetching page 113\n",
      "Fetching page 114\n",
      "Fetching page 115\n",
      "Fetching page 116\n",
      "Fetching page 117\n",
      "Fetching page 118\n",
      "Fetching page 119\n",
      "Fetching page 120\n",
      "Fetching page 121\n",
      "Fetching page 122\n",
      "Fetching page 123\n",
      "Fetching page 124\n",
      "Fetching page 125\n",
      "Fetching page 126\n",
      "Fetching page 127\n",
      "Fetching page 128\n",
      "Fetching page 129\n",
      "Fetching page 130\n",
      "Fetching page 131\n",
      "Fetching page 132\n",
      "Fetching page 133\n",
      "Fetching page 134\n",
      "Fetching page 135\n",
      "Fetching page 136\n",
      "Fetching page 137\n",
      "Fetching page 138\n",
      "Fetching page 139\n",
      "Fetching page 140\n",
      "Fetching page 141\n",
      "Fetching page 142\n",
      "Fetching page 143\n",
      "Fetching page 144\n",
      "Fetching page 145\n",
      "Fetching page 146\n",
      "Fetching page 147\n",
      "Fetching page 148\n",
      "Fetching page 149\n",
      "Fetching page 150\n",
      "Fetching page 151\n",
      "Fetching page 152\n",
      "Fetching page 153\n",
      "Fetching page 154\n",
      "Fetching page 155\n",
      "Fetching page 156\n",
      "Fetching page 157\n",
      "Fetching page 158\n",
      "Fetching page 159\n",
      "Fetching page 160\n",
      "Fetching page 161\n",
      "Fetching page 162\n",
      "Fetching page 163\n",
      "Fetching page 164\n",
      "Fetching page 165\n",
      "Fetching page 166\n",
      "Fetching page 167\n",
      "Fetching page 168\n",
      "Fetching page 169\n",
      "Fetching page 170\n",
      "Fetching page 171\n",
      "Fetching page 172\n",
      "Fetching page 173\n",
      "Fetching page 174\n",
      "Fetching page 175\n",
      "Fetching page 176\n",
      "Fetching page 177\n",
      "Fetching page 178\n",
      "Fetching page 179\n",
      "Fetching page 180\n",
      "Fetching page 181\n",
      "Fetching page 182\n",
      "Fetching page 183\n",
      "Fetching page 184\n",
      "Fetching page 185\n",
      "Fetching page 186\n",
      "Fetching page 187\n",
      "Fetching page 188\n",
      "Fetching page 189\n",
      "Fetching page 190\n",
      "Fetching page 191\n",
      "Fetching page 192\n",
      "Fetching page 193\n",
      "Fetching page 194\n",
      "Fetching page 195\n",
      "Fetching page 196\n",
      "Fetching page 197\n",
      "Fetching page 198\n",
      "Fetching page 199\n",
      "Fetching page 200\n",
      "Fetching page 201\n",
      "Fetching page 202\n",
      "Fetching page 203\n",
      "Fetching page 204\n",
      "Fetching page 205\n",
      "Fetching page 206\n",
      "Fetching page 207\n",
      "Fetching page 208\n",
      "Fetching page 209\n",
      "Fetching page 210\n",
      "Fetching page 211\n",
      "Fetching page 212\n",
      "Fetching page 213\n",
      "Fetching page 214\n",
      "Fetching page 215\n",
      "Fetching page 216\n",
      "Fetching page 217\n",
      "Fetching page 218\n",
      "Fetching page 219\n",
      "Fetching page 220\n",
      "Fetching page 221\n",
      "Fetching page 222\n",
      "Fetching page 223\n",
      "Fetching page 224\n",
      "Fetching page 225\n",
      "Fetching page 226\n",
      "Fetching page 227\n",
      "Fetching page 228\n",
      "Fetching page 229\n",
      "Fetching page 230\n",
      "Fetching page 231\n",
      "Fetching page 232\n",
      "Fetching page 233\n",
      "Fetching page 234\n",
      "Fetching page 235\n",
      "Fetching page 236\n",
      "Fetching page 237\n",
      "Fetching page 238\n",
      "Fetching page 239\n",
      "Fetching page 240\n",
      "Fetching page 241\n",
      "Fetching page 242\n",
      "Fetching page 243\n",
      "Fetching page 244\n",
      "Fetching page 245\n",
      "Fetching page 246\n",
      "Fetching page 247\n",
      "Fetching page 248\n",
      "Fetching page 249\n",
      "Fetching page 250\n",
      "Fetching page 251\n",
      "Fetching page 252\n",
      "Fetching page 253\n",
      "Fetching page 254\n",
      "Fetching page 255\n",
      "Fetching page 256\n",
      "Fetching page 257\n",
      "Fetching page 258\n",
      "Fetching page 259\n",
      "Fetching page 260\n",
      "Fetching page 261\n",
      "Fetching page 262\n",
      "Fetching page 263\n",
      "Fetching page 264\n",
      "Fetching page 265\n",
      "Fetching page 266\n",
      "Fetching page 267\n",
      "Fetching page 268\n",
      "Fetching page 269\n",
      "Fetching page 270\n",
      "Fetching page 271\n",
      "Fetching page 272\n",
      "Fetching page 273\n",
      "Fetching page 274\n",
      "Fetching page 275\n",
      "Fetching page 276\n",
      "Fetching page 277\n",
      "Fetching page 278\n",
      "Fetching page 279\n",
      "Fetching page 280\n",
      "Fetching page 281\n",
      "Fetching page 282\n",
      "Fetching page 283\n",
      "Fetching page 284\n",
      "Fetching page 285\n",
      "Fetching page 286\n",
      "Fetching page 287\n",
      "Fetching page 288\n",
      "Fetching page 289\n",
      "Fetching page 290\n",
      "Fetching page 291\n",
      "Fetching page 292\n",
      "Fetching page 293\n",
      "Fetching page 294\n",
      "Fetching page 295\n",
      "Fetching page 296\n",
      "Fetching page 297\n",
      "Fetching page 298\n",
      "Fetching page 299\n",
      "Fetching page 300\n",
      "Fetching page 301\n",
      "Fetching page 302\n",
      "Fetching page 303\n",
      "Fetching page 304\n",
      "Fetching page 305\n",
      "Fetching page 306\n",
      "Fetching page 307\n",
      "Fetching page 308\n",
      "Fetching page 309\n",
      "Fetching page 310\n",
      "Fetching page 311\n",
      "Fetching page 312\n",
      "Fetching page 313\n",
      "Fetching page 314\n",
      "Fetching page 315\n",
      "Fetching page 316\n",
      "Fetching page 317\n",
      "Fetching page 318\n",
      "Fetching page 319\n",
      "Fetching page 320\n",
      "Fetching page 321\n",
      "Fetching page 322\n",
      "Fetching page 323\n",
      "Fetching page 324\n",
      "Fetching page 325\n",
      "Fetching page 326\n",
      "Fetching page 327\n",
      "Fetching page 328\n",
      "Fetching page 329\n",
      "Fetching page 330\n",
      "Fetching page 331\n",
      "Fetching page 332\n",
      "Fetching page 333\n",
      "Fetching page 334\n",
      "Fetching page 335\n",
      "Fetching page 336\n",
      "Fetching page 337\n",
      "Fetching page 338\n",
      "Fetching page 339\n",
      "Fetching page 340\n",
      "Fetching page 341\n",
      "Fetching page 342\n",
      "Fetching page 343\n",
      "Fetching page 344\n",
      "Fetching page 345\n",
      "Fetching page 346\n",
      "Fetching page 347\n",
      "Fetching page 348\n",
      "Fetching page 349\n",
      "Fetching page 350\n",
      "Fetching page 351\n",
      "Fetching page 352\n",
      "Fetching page 353\n",
      "Fetching page 354\n",
      "Fetching page 355\n",
      "Fetching page 356\n",
      "Fetching page 357\n",
      "Updated news data uploaded to S3: news-data/news_history_1yr.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# AWS S3 Setup\n",
    "#S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "#S3_KEY = \"news-data/historical_news_1yr.csv\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"news-data/news_history_1yr.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# API Setup\n",
    "API_KEY = 'hDdU807PlusyraTBnNgkkm2gFuPtkZ9F'\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "tickers_str = ','.join(tickers)\n",
    "\n",
    "def load_existing_news():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_df = pd.read_csv(response['Body'], parse_dates=[\"publishedDate\"])\n",
    "        print(f\"Loaded existing news data: {len(existing_df)} articles\")\n",
    "        return existing_df\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing news file found in S3.\")\n",
    "        return None\n",
    "\n",
    "def upload_to_s3(df):\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=buffer.getvalue())\n",
    "    print(f\"Updated news data uploaded to S3: {S3_KEY}\")\n",
    "\n",
    "def fetch_news(from_date, to_date):\n",
    "    page = 0\n",
    "    limit = 100\n",
    "    all_news = []\n",
    "\n",
    "    print(f\"Fetching news from {from_date} to {to_date}\")\n",
    "    while True:\n",
    "        page += 1\n",
    "        print(f\"Fetching page {page}\")\n",
    "        url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "        params = {\n",
    "            \"tickers\": tickers_str,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": limit,\n",
    "            \"page\": page,\n",
    "            \"apikey\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_batch = response.json()\n",
    "            if not news_batch:\n",
    "                break\n",
    "            all_news.extend(news_batch)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed with status {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    if all_news:\n",
    "        df = pd.DataFrame(all_news)\n",
    "        df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "        df = df.sort_values(by=[\"symbol\", \"publishedDate\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No news returned.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main logic\n",
    "existing_news = load_existing_news()\n",
    "\n",
    "if existing_news is not None:\n",
    "    last_date = existing_news[\"publishedDate\"].max()\n",
    "    start_date = last_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Updating from last saved date: {start_date}\")\n",
    "else:\n",
    "    start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "    print(f\"First-time fetch from: {start_date}\")\n",
    "    \n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch new data\n",
    "new_news_df = fetch_news(start_date, end_date)\n",
    "\n",
    "# Merge & upload\n",
    "if not new_news_df.empty:\n",
    "    if existing_news is not None:\n",
    "        combined_df = pd.concat([existing_news, new_news_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=[\"symbol\", \"publishedDate\", \"title\"], inplace=True)\n",
    "    else:\n",
    "        combined_df = new_news_df\n",
    "\n",
    "    upload_to_s3(combined_df)\n",
    "else:\n",
    "    print(\"News already up to date. No new records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e604d16-2b20-40d1-9676-82c560415eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:23:19.363234Z",
     "iopub.status.busy": "2025-04-11T16:23:19.362443Z",
     "iopub.status.idle": "2025-04-11T16:23:19.455648Z",
     "shell.execute_reply": "2025-04-11T16:23:19.454707Z",
     "shell.execute_reply.started": "2025-04-11T16:23:19.363199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Deleting all items from: realtimedata...\n",
      "âœ… Deleted 0 items from realtimedata.\n",
      "ðŸ§¹ Deleting all items from: news_tracker...\n",
      "âœ… Deleted 0 items from news_tracker.\n",
      "ðŸ§¹ Deleting all items from: sentiment_tracker...\n",
      "âœ… Deleted 0 items from sentiment_tracker.\n",
      "ðŸ§¹ Deleting all items from: realtime_predictions...\n",
      "âœ… Deleted 0 items from realtime_predictions.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# List of DynamoDB table names\n",
    "TABLE_NAMES = [\n",
    "    \"realtimedata\",\n",
    "    \"news_tracker\",\n",
    "    \"sentiment_tracker\",\n",
    "    \"realtime_predictions\"\n",
    "]\n",
    "\n",
    "# Initialize DynamoDB\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "\n",
    "def delete_all_items(table_name):\n",
    "    table = dynamodb.Table(table_name)\n",
    "    print(f\"ðŸ§¹ Deleting all items from: {table_name}...\")\n",
    "\n",
    "    # Scan all items\n",
    "    response = table.scan()\n",
    "    items = response.get(\"Items\", [])\n",
    "\n",
    "    # Loop and delete items one by one\n",
    "    with table.batch_writer() as batch:\n",
    "        for item in items:\n",
    "            # Assume the first key is the primary key\n",
    "            key_schema = table.key_schema\n",
    "            key = {k['AttributeName']: item[k['AttributeName']] for k in key_schema}\n",
    "            batch.delete_item(Key=key)\n",
    "\n",
    "    print(f\"âœ… Deleted {len(items)} items from {table_name}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for table in TABLE_NAMES:\n",
    "        delete_all_items(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4b5a03a-5d53-4370-b45f-cf70c31f3f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:06:05.176505Z",
     "iopub.status.busy": "2025-04-11T17:06:05.176008Z",
     "iopub.status.idle": "2025-04-11T17:06:05.767767Z",
     "shell.execute_reply": "2025-04-11T17:06:05.766811Z",
     "shell.execute_reply.started": "2025-04-11T17:06:05.176462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total Unique Stocks in 'realtimedata': 7\n",
      "ðŸ§¾ Stock Symbols: ['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# DynamoDB setup\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "table = dynamodb.Table(\"realtimedata\")\n",
    "\n",
    "# Scan the table and collect stock symbols\n",
    "def get_unique_stocks():\n",
    "    unique_stocks = set()\n",
    "    response = table.scan(ProjectionExpression=\"Stock\")\n",
    "    items = response.get(\"Items\", [])\n",
    "    \n",
    "    for item in items:\n",
    "        unique_stocks.add(item[\"Stock\"])\n",
    "    \n",
    "    # Handle pagination\n",
    "    while \"LastEvaluatedKey\" in response:\n",
    "        response = table.scan(\n",
    "            ProjectionExpression=\"Stock\",\n",
    "            ExclusiveStartKey=response[\"LastEvaluatedKey\"]\n",
    "        )\n",
    "        items = response.get(\"Items\", [])\n",
    "        for item in items:\n",
    "            unique_stocks.add(item[\"Stock\"])\n",
    "\n",
    "    return unique_stocks\n",
    "\n",
    "# Run and print\n",
    "stocks = get_unique_stocks()\n",
    "print(f\"âœ… Total Unique Stocks in 'realtimedata': {len(stocks)}\")\n",
    "print(f\"ðŸ§¾ Stock Symbols: {sorted(stocks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e3f941-f94d-4a22-bfe8-2939fd0236fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:23:25.727298Z",
     "iopub.status.busy": "2025-04-11T17:23:25.726615Z",
     "iopub.status.idle": "2025-04-11T17:23:27.940463Z",
     "shell.execute_reply": "2025-04-11T17:23:27.938789Z",
     "shell.execute_reply.started": "2025-04-11T17:23:25.727265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Fetching data from DynamoDB...\n",
      "ðŸ“¤ Uploading to S3...\n",
      "âœ… Data exported to s3://stock-market-data-uofc/dashboard-data/realtimedata_export.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# AWS Setup\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "table = dynamodb.Table(\"realtimedata\")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"dashboard-data/realtimedata_export.csv\"\n",
    "\n",
    "# Fetch all rows from DynamoDB\n",
    "def fetch_all_data():\n",
    "    items = []\n",
    "    response = table.scan()\n",
    "    items.extend(response[\"Items\"])\n",
    "\n",
    "    # Handle pagination\n",
    "    while \"LastEvaluatedKey\" in response:\n",
    "        response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n",
    "        items.extend(response[\"Items\"])\n",
    "\n",
    "    return items\n",
    "\n",
    "# Convert and upload\n",
    "def export_to_s3():\n",
    "    print(\"ðŸ“¥ Fetching data from DynamoDB...\")\n",
    "    data = fetch_all_data()\n",
    "    if not data:\n",
    "        print(\"âš ï¸ No data found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    print(\"ðŸ“¤ Uploading to S3...\")\n",
    "    s3.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer.getvalue())\n",
    "    print(f\"âœ… Data exported to s3://{S3_BUCKET}/{S3_KEY}\")\n",
    "\n",
    "# Run export\n",
    "if __name__ == \"__main__\":\n",
    "    export_to_s3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd85206-12b0-4f8a-842e-e4823bb369db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

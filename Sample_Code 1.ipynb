{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to fetch Historical stock data for past 1 year with the interval of 1 day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS S3 Configuration\n",
    "#S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "#S3_KEY = \"historical-stock-data/stock_1y_1d.csv\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"historical-data/stock_5y_1d.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# List of stocks to fetch\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "# Fetch stock data from a given start date\n",
    "def fetch_stock_data(start_date):\n",
    "    all_stock_data = []\n",
    "    for stock in stocks:\n",
    "        print(f\"Fetching {stock} from {start_date}...\")\n",
    "        ticker = yf.Ticker(stock)\n",
    "        history = ticker.history(start=start_date, interval=\"1d\")\n",
    "        history.reset_index(inplace=True)\n",
    "        history[\"Stock\"] = stock\n",
    "        history = history[[\"Stock\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        all_stock_data.append(history)\n",
    "    return pd.concat(all_stock_data, ignore_index=True)\n",
    "\n",
    "# Load data from S3\n",
    "def load_existing_data():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_data = pd.read_csv(response['Body'], parse_dates=[\"Date\"])\n",
    "        print(f\"Loaded existing data: {len(existing_data)} rows\")\n",
    "        return existing_data\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing data found. Fetching fresh...\")\n",
    "        return None\n",
    "\n",
    "# Save DataFrame to S3\n",
    "def upload_to_s3(df):\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer)\n",
    "    print(f\"Uploaded to S3: {S3_KEY}\")\n",
    "\n",
    "# Main logic\n",
    "existing_df = load_existing_data()\n",
    "\n",
    "if existing_df is not None:\n",
    "    last_date = existing_df[\"Date\"].max().date()\n",
    "    next_date = last_date + pd.Timedelta(days=1)\n",
    "    new_data = fetch_stock_data(start_date=next_date)\n",
    "    combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"Stock\", \"Date\"], inplace=True)\n",
    "else:\n",
    "    start_date = (datetime.now() - pd.Timedelta(days=365)).date()\n",
    "    combined_df = fetch_stock_data(start_date=start_date)\n",
    "\n",
    "upload_to_s3(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to fetch  Historical stock data of past 60 days with the interval of 5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import timezone\n",
    "start_time = datetime.now(timezone.utc) - timedelta(days=60)\n",
    "\n",
    "# AWS S3 Configuration\n",
    "#S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "S3_BUCKET = \"stock-market-data-uofc\"\n",
    "S3_KEY = \"real-time-stock-data/stock_60d_5m.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Stocks to fetch\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "# Function to fetch 5-minute interval data for the last 60 days\n",
    "def fetch_stock_data(start_datetime):\n",
    "    all_data = []\n",
    "    for stock in stocks:\n",
    "        print(f\"Fetching {stock} from {start_datetime}...\")\n",
    "        ticker = yf.Ticker(stock)\n",
    "        history = ticker.history(period=\"60d\", interval=\"5m\")\n",
    "        history.reset_index(inplace=True)\n",
    "        history[\"Stock\"] = stock\n",
    "        history = history.rename(columns={\"Datetime\": \"Date\"})\n",
    "        history = history[[\"Stock\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        history = history[history[\"Date\"] > start_datetime]\n",
    "        all_data.append(history)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Load existing data from S3\n",
    "def load_existing_data():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_df = pd.read_csv(response['Body'], parse_dates=[\"Date\"])\n",
    "        print(f\"Loaded existing data: {len(existing_df)} rows\")\n",
    "        return existing_df\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing data found in S3.\")\n",
    "        return None\n",
    "\n",
    "# Upload DataFrame to S3\n",
    "def upload_to_s3(df):\n",
    "    csv_buffer = df.to_csv(index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=csv_buffer)\n",
    "    print(f\"Uploaded updated dataset to S3: {S3_KEY}\")\n",
    "\n",
    "# Main logic\n",
    "existing_df = load_existing_data()\n",
    "\n",
    "if existing_df is not None:\n",
    "    last_time = existing_df[\"Date\"].max()\n",
    "    new_data = fetch_stock_data(start_datetime=last_time)\n",
    "    combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"Stock\", \"Date\"], inplace=True)\n",
    "else:\n",
    "    print(\"First-time fetch for last 60 days (5m)...\")\n",
    "    start_time = datetime.now() - timedelta(days=60)\n",
    "    combined_df = fetch_stock_data(start_datetime=start_time)\n",
    "\n",
    "upload_to_s3(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to fetch the Historical News data for past 1 year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# AWS S3 Setup\n",
    "S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "S3_KEY = \"news-data/historical_news_1yr.csv\"\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# API Setup\n",
    "API_KEY = 'hDdU807PlusyraTBnNgkkm2gFuPtkZ9F'\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "tickers_str = ','.join(tickers)\n",
    "\n",
    "def load_existing_news():\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
    "        existing_df = pd.read_csv(response['Body'], parse_dates=[\"publishedDate\"])\n",
    "        print(f\"Loaded existing news data: {len(existing_df)} articles\")\n",
    "        return existing_df\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(\"No existing news file found in S3.\")\n",
    "        return None\n",
    "\n",
    "def upload_to_s3(df):\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=S3_KEY, Body=buffer.getvalue())\n",
    "    print(f\"Updated news data uploaded to S3: {S3_KEY}\")\n",
    "\n",
    "def fetch_news(from_date, to_date):\n",
    "    page = 0\n",
    "    limit = 100\n",
    "    all_news = []\n",
    "\n",
    "    print(f\"Fetching news from {from_date} to {to_date}\")\n",
    "    while True:\n",
    "        page += 1\n",
    "        print(f\"Fetching page {page}\")\n",
    "        url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "        params = {\n",
    "            \"tickers\": tickers_str,\n",
    "            \"from\": from_date,\n",
    "            \"to\": to_date,\n",
    "            \"limit\": limit,\n",
    "            \"page\": page,\n",
    "            \"apikey\": API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_batch = response.json()\n",
    "            if not news_batch:\n",
    "                break\n",
    "            all_news.extend(news_batch)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed with status {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    if all_news:\n",
    "        df = pd.DataFrame(all_news)\n",
    "        df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "        df = df.sort_values(by=[\"symbol\", \"publishedDate\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No news returned.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main logic\n",
    "existing_news = load_existing_news()\n",
    "\n",
    "if existing_news is not None:\n",
    "    last_date = existing_news[\"publishedDate\"].max()\n",
    "    start_date = last_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Updating from last saved date: {start_date}\")\n",
    "else:\n",
    "    start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "    print(f\"First-time fetch from: {start_date}\")\n",
    "    \n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch new data\n",
    "new_news_df = fetch_news(start_date, end_date)\n",
    "\n",
    "# Merge & upload\n",
    "if not new_news_df.empty:\n",
    "    if existing_news is not None:\n",
    "        combined_df = pd.concat([existing_news, new_news_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=[\"symbol\", \"publishedDate\", \"title\"], inplace=True)\n",
    "    else:\n",
    "        combined_df = new_news_df\n",
    "\n",
    "    upload_to_s3(combined_df)\n",
    "else:\n",
    "    print(\"News already up to date. No new records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code to extract the real time news at the interval of 1 hour will be the same as shown above, which will give the latest news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionally, we will extract the real time stock data every 5 minutes, attach relative news with it, perform preprocessing and sentiment analysis, and finally store it in the database to train the maodel on the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Sentiment analysis, pre processing and merging the stock news data with the news data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently saving the merged data in s3 bucket, but for the real project, we will be storing the merged dataset in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load FinBERT\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# AWS S3 Config\n",
    "bucket = \"data608-2025-stock-market-data\"\n",
    "news_key = \"historical-data/1_year_stock_news.csv\"\n",
    "stock_key = \"historical-data/1_year_stock_data.csv\"\n",
    "output_key = \"processed-data/merged_1yr_preprocessed.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Load news data from S3\n",
    "news_obj = s3.get_object(Bucket=bucket, Key=news_key)\n",
    "df = pd.read_csv(news_obj['Body'])\n",
    "df = df.drop(columns=[\"image\", \"site\", \"url\"], errors=\"ignore\")\n",
    "text_column = \"text\"\n",
    "\n",
    "# Define FinBERT scoring function\n",
    "def get_finbert_sentiment(text_column):\n",
    "    try:\n",
    "        inputs = tokenizer(text_column, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            sentiment_scores = probs[0].tolist()\n",
    "            labels = ['negative', 'neutral', 'positive']\n",
    "            sentiment_label = labels[torch.argmax(probs)]\n",
    "            return sentiment_label, sentiment_scores\n",
    "    except:\n",
    "        return \"error\", [None, None, None]\n",
    "\n",
    "tqdm.pandas()\n",
    "df[['sentiment_label', 'sentiment_score']] = df[text_column].progress_apply(\n",
    "    lambda x: pd.Series(get_finbert_sentiment(str(x)))\n",
    ")\n",
    "\n",
    "df[['neg_score', 'neu_score', 'pos_score']] = pd.DataFrame(df['sentiment_score'].tolist(), index=df.index)\n",
    "df['sentiment_weighted'] = (-1 * df['neg_score']) + (0 * df['neu_score']) + (1 * df['pos_score'])\n",
    "df.drop(columns=['sentiment_score', 'neg_score', 'neu_score', 'pos_score'], inplace=True)\n",
    "\n",
    "# Aggregate sentiment by day and symbol\n",
    "df['date'] = pd.to_datetime(df['publishedDate'], errors='coerce').dt.date\n",
    "agg_df = df.groupby(['symbol', 'date']).agg({\n",
    "    'sentiment_weighted': ['mean', 'min', 'max'],\n",
    "    'sentiment_label': 'count'\n",
    "}).reset_index()\n",
    "agg_df.columns = ['symbol', 'date', 'sentiment_mean', 'sentiment_min', 'sentiment_max', 'sentiment_count']\n",
    "agg_df.rename(columns={'symbol': 'Stock'}, inplace=True)\n",
    "\n",
    "# Load stock data from S3\n",
    "stock_obj = s3.get_object(Bucket=bucket, Key=stock_key)\n",
    "stock_df = pd.read_csv(stock_obj['Body'])\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'], utc=True, errors='coerce')\n",
    "stock_df['Date'] = stock_df['Date'].dt.tz_convert(None)\n",
    "stock_df['date'] = stock_df['Date'].dt.date\n",
    "\n",
    "# Merge and save\n",
    "merged_df = pd.merge(stock_df, agg_df, how=\"left\", on=[\"Stock\", \"date\"])\n",
    "\n",
    "# Save back to S3\n",
    "csv_buffer = StringIO()\n",
    "merged_df.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket, Key=output_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(\"Merged data saved to S3:\", output_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess, sentiment, merge and store it to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Preprocess + Sentiment + Merge + Store to RDS\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ========== 1. Load FinBERT ==========\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# ========== 2. AWS S3 Config ==========\n",
    "bucket = \"data608-2025-stock-market-data\"\n",
    "news_key = \"historical-data/1_year_stock_news.csv\"\n",
    "stock_key = \"historical-data/1_year_stock_data.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# ========== 3. Load News Data ==========\n",
    "news_obj = s3.get_object(Bucket=bucket, Key=news_key)\n",
    "df_news = pd.read_csv(news_obj[\"Body\"])\n",
    "df_news = df_news.drop(columns=[\"image\", \"site\", \"url\"], errors=\"ignore\")\n",
    "\n",
    "# ========== 4. Apply FinBERT ==========\n",
    "def get_finbert_sentiment(text_column):\n",
    "    try:\n",
    "        inputs = tokenizer(text_column, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            sentiment_scores = probs[0].tolist()\n",
    "            labels = ['negative', 'neutral', 'positive']\n",
    "            sentiment_label = labels[torch.argmax(probs)]\n",
    "            return sentiment_label, sentiment_scores\n",
    "    except:\n",
    "        return \"error\", [None, None, None]\n",
    "\n",
    "tqdm.pandas()\n",
    "df_news[[\"sentiment_label\", \"sentiment_score\"]] = df_news[\"text\"].progress_apply(\n",
    "    lambda x: pd.Series(get_finbert_sentiment(str(x)))\n",
    ")\n",
    "\n",
    "df_news[[\"neg_score\", \"neu_score\", \"pos_score\"]] = pd.DataFrame(df_news[\"sentiment_score\"].tolist(), index=df_news.index)\n",
    "df_news[\"sentiment_weighted\"] = (-1 * df_news[\"neg_score\"]) + (0 * df_news[\"neu_score\"]) + (1 * df_news[\"pos_score\"])\n",
    "df_news.drop(columns=[\"sentiment_score\", \"neg_score\", \"neu_score\", \"pos_score\"], inplace=True)\n",
    "\n",
    "# ========== 5. Aggregate News ==========\n",
    "df_news[\"date\"] = pd.to_datetime(df_news[\"publishedDate\"], errors=\"coerce\").dt.date\n",
    "agg_df = df_news.groupby([\"symbol\", \"date\"]).agg({\n",
    "    \"sentiment_weighted\": [\"mean\", \"min\", \"max\"],\n",
    "    \"sentiment_label\": \"count\"\n",
    "}).reset_index()\n",
    "\n",
    "agg_df.columns = [\"symbol\", \"date\", \"sentiment_mean\", \"sentiment_min\", \"sentiment_max\", \"sentiment_count\"]\n",
    "agg_df.rename(columns={\"symbol\": \"Stock\"}, inplace=True)\n",
    "\n",
    "# ========== 6. Load Stock Data ==========\n",
    "stock_obj = s3.get_object(Bucket=bucket, Key=stock_key)\n",
    "df_stock = pd.read_csv(stock_obj[\"Body\"])\n",
    "df_stock[\"Date\"] = pd.to_datetime(df_stock[\"Date\"], utc=True, errors=\"coerce\")\n",
    "df_stock[\"Date\"] = df_stock[\"Date\"].dt.tz_convert(None)\n",
    "df_stock[\"date\"] = df_stock[\"Date\"].dt.date\n",
    "\n",
    "# ========== 7. Merge ==========\n",
    "merged_df = pd.merge(df_stock, agg_df, how=\"left\", on=[\"Stock\", \"date\"])\n",
    "\n",
    "# ========== 8. Upload to PostgreSQL RDS ==========\n",
    "# Replace these with your credentials\n",
    "rds_host = \"your-rds-endpoint.rds.amazonaws.com\"\n",
    "rds_port = \"5432\"\n",
    "rds_db = \"postgres\"\n",
    "rds_user = \"admin\"\n",
    "rds_password = \"your_password\"\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg2://{rds_user}:{rds_password}@{rds_host}:{rds_port}/{rds_db}')\n",
    "merged_df.to_sql(\"historical_stock_sentiment\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"âœ… Final merged dataset uploaded to RDS table: historical_stock_sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Daily Stock + News Update Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# ðŸ”§ Config\n",
    "API_KEY = 'YOUR_FINANCIAL_MODELING_PREP_KEY'\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "\n",
    "# PostgreSQL RDS Config\n",
    "RDS_HOST = 'your-db-endpoint.rds.amazonaws.com'\n",
    "RDS_DB = 'your_db'\n",
    "RDS_USER = 'your_user'\n",
    "RDS_PASS = 'your_password'\n",
    "\n",
    "# â³ Date\n",
    "yesterday = datetime.utcnow().date() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "# ðŸ”½ Fetch Daily Stock Data\n",
    "import yfinance as yf\n",
    "stock_rows = []\n",
    "for symbol in stocks:\n",
    "    data = yf.Ticker(symbol).history(period=\"2d\", interval=\"1d\").reset_index()\n",
    "    if len(data) > 1:\n",
    "        row = data.iloc[-1]\n",
    "        stock_rows.append({\n",
    "            \"Stock\": symbol,\n",
    "            \"Date\": row[\"Date\"].date(),\n",
    "            \"Open\": row[\"Open\"],\n",
    "            \"High\": row[\"High\"],\n",
    "            \"Low\": row[\"Low\"],\n",
    "            \"Close\": row[\"Close\"],\n",
    "            \"Volume\": row[\"Volume\"]\n",
    "        })\n",
    "stock_df = pd.DataFrame(stock_rows)\n",
    "\n",
    "# ðŸ“° Fetch News from Financial Modeling Prep\n",
    "all_news = []\n",
    "page = 0\n",
    "limit = 100\n",
    "while True:\n",
    "    page += 1\n",
    "    url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "    params = {\n",
    "        \"tickers\": ','.join(stocks),\n",
    "        \"from\": yesterday_str,\n",
    "        \"to\": yesterday_str,\n",
    "        \"limit\": limit,\n",
    "        \"page\": page,\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200 or not response.json():\n",
    "        break\n",
    "    all_news.extend(response.json())\n",
    "\n",
    "# â¬‡ï¸ Preprocess + Sentiment\n",
    "if all_news:\n",
    "    df = pd.DataFrame(all_news)\n",
    "    df = df[[\"symbol\", \"publishedDate\", \"title\"]].copy()\n",
    "    df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    df[\"text\"] = df[\"title\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def get_sentiment(text):\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = softmax(logits, dim=1)[0]\n",
    "                label = ['negative', 'neutral', 'positive'][torch.argmax(probs)]\n",
    "                score = (-1 * probs[0].item()) + (0 * probs[1].item()) + (1 * probs[2].item())\n",
    "                return label, score\n",
    "        except:\n",
    "            return \"error\", 0.0\n",
    "\n",
    "    df[['sentiment_label', 'sentiment_score']] = df['text'].apply(lambda x: pd.Series(get_sentiment(str(x))))\n",
    "    \n",
    "    # Group sentiment\n",
    "    sentiment_df = df.groupby(['symbol', 'date']).agg(\n",
    "        sentiment_mean=('sentiment_score', 'mean'),\n",
    "        sentiment_min=('sentiment_score', 'min'),\n",
    "        sentiment_max=('sentiment_score', 'max'),\n",
    "        sentiment_count=('sentiment_label', 'count')\n",
    "    ).reset_index()\n",
    "    sentiment_df.rename(columns={\"symbol\": \"Stock\"}, inplace=True)\n",
    "else:\n",
    "    # If no news, create neutral sentiment\n",
    "    sentiment_df = pd.DataFrame({\n",
    "        \"Stock\": stocks,\n",
    "        \"date\": [yesterday]*len(stocks),\n",
    "        \"sentiment_mean\": [0.0]*len(stocks),\n",
    "        \"sentiment_min\": [0.0]*len(stocks),\n",
    "        \"sentiment_max\": [0.0]*len(stocks),\n",
    "        \"sentiment_count\": [0]*len(stocks)\n",
    "    })\n",
    "\n",
    "# ðŸ”— Merge\n",
    "merged_df = pd.merge(stock_df, sentiment_df, left_on=[\"Stock\", \"Date\"], right_on=[\"Stock\", \"date\"], how=\"left\")\n",
    "merged_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "# ðŸ’¾ Store in PostgreSQL RDS\n",
    "conn = psycopg2.connect(\n",
    "    host=RDS_HOST,\n",
    "    database=RDS_DB,\n",
    "    user=RDS_USER,\n",
    "    password=RDS_PASS\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO stock_sentiment_daily (\n",
    "            stock, date, open, high, low, close, volume,\n",
    "            sentiment_mean, sentiment_min, sentiment_max, sentiment_count\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (stock, date) DO NOTHING;\n",
    "    \"\"\", (\n",
    "        row['Stock'], row['Date'], row['Open'], row['High'], row['Low'],\n",
    "        row['Close'], row['Volume'],\n",
    "        row['sentiment_mean'], row['sentiment_min'],\n",
    "        row['sentiment_max'], row['sentiment_count']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Real-time stock + sentiment row inserted for\", yesterday_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the same, but will avoid duplication in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import yfinance as yf\n",
    "\n",
    "# ðŸ”§ Config\n",
    "API_KEY = 'YOUR_FINANCIAL_MODELING_PREP_KEY'\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "\n",
    "# PostgreSQL RDS Config\n",
    "RDS_HOST = 'your-db-endpoint.rds.amazonaws.com'\n",
    "RDS_DB = 'your_db'\n",
    "RDS_USER = 'your_user'\n",
    "RDS_PASS = 'your_password'\n",
    "\n",
    "# â³ Date\n",
    "yesterday = datetime.utcnow().date() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“ˆ Step 1: Fetch Stock Data (Daily)\n",
    "stock_rows = []\n",
    "for symbol in stocks:\n",
    "    data = yf.Ticker(symbol).history(period=\"2d\", interval=\"1d\").reset_index()\n",
    "    if len(data) > 1:\n",
    "        row = data.iloc[-1]\n",
    "        stock_rows.append({\n",
    "            \"Stock\": symbol,\n",
    "            \"Date\": row[\"Date\"].date(),\n",
    "            \"Open\": row[\"Open\"],\n",
    "            \"High\": row[\"High\"],\n",
    "            \"Low\": row[\"Low\"],\n",
    "            \"Close\": row[\"Close\"],\n",
    "            \"Volume\": row[\"Volume\"]\n",
    "        })\n",
    "\n",
    "stock_df = pd.DataFrame(stock_rows)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“° Step 2: Fetch News (for yesterday only)\n",
    "all_news = []\n",
    "page = 0\n",
    "limit = 100\n",
    "while True:\n",
    "    page += 1\n",
    "    url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "    params = {\n",
    "        \"tickers\": ','.join(stocks),\n",
    "        \"from\": yesterday_str,\n",
    "        \"to\": yesterday_str,\n",
    "        \"limit\": limit,\n",
    "        \"page\": page,\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200 or not response.json():\n",
    "        break\n",
    "    all_news.extend(response.json())\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ¤– Step 3: Sentiment Analysis\n",
    "if all_news:\n",
    "    df = pd.DataFrame(all_news)\n",
    "    df = df[[\"symbol\", \"publishedDate\", \"title\"]].copy()\n",
    "    df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "    df[\"date\"] = df[\"publishedDate\"].dt.date\n",
    "    df[\"text\"] = df[\"title\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def get_sentiment(text):\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = softmax(logits, dim=1)[0]\n",
    "                score = (-1 * probs[0].item()) + (0 * probs[1].item()) + (1 * probs[2].item())\n",
    "                return score\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    df[\"sentiment_score\"] = df[\"text\"].apply(lambda x: get_sentiment(str(x)))\n",
    "\n",
    "    # Aggregate sentiment\n",
    "    sentiment_df = df.groupby(['symbol', 'date']).agg(\n",
    "        sentiment_mean=('sentiment_score', 'mean'),\n",
    "        sentiment_min=('sentiment_score', 'min'),\n",
    "        sentiment_max=('sentiment_score', 'max'),\n",
    "        sentiment_count=('sentiment_score', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    sentiment_df.rename(columns={\"symbol\": \"Stock\"}, inplace=True)\n",
    "else:\n",
    "    # Default to neutral sentiment if no news\n",
    "    sentiment_df = pd.DataFrame({\n",
    "        \"Stock\": stocks,\n",
    "        \"date\": [yesterday] * len(stocks),\n",
    "        \"sentiment_mean\": [0.0] * len(stocks),\n",
    "        \"sentiment_min\": [0.0] * len(stocks),\n",
    "        \"sentiment_max\": [0.0] * len(stocks),\n",
    "        \"sentiment_count\": [0] * len(stocks)\n",
    "    })\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ”— Step 4: Merge\n",
    "merged_df = pd.merge(stock_df, sentiment_df, left_on=[\"Stock\", \"Date\"], right_on=[\"Stock\", \"date\"], how=\"left\")\n",
    "merged_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ’¾ Step 5: Store in PostgreSQL (UPSERT)\n",
    "conn = psycopg2.connect(\n",
    "    host=RDS_HOST,\n",
    "    database=RDS_DB,\n",
    "    user=RDS_USER,\n",
    "    password=RDS_PASS\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Make sure UNIQUE constraint on (stock, date) is present in your table\n",
    "for _, row in merged_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO stock_sentiment_daily (\n",
    "            stock, date, open, high, low, close, volume,\n",
    "            sentiment_mean, sentiment_min, sentiment_max, sentiment_count\n",
    "        )\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (stock, date) DO UPDATE SET\n",
    "            open = EXCLUDED.open,\n",
    "            high = EXCLUDED.high,\n",
    "            low = EXCLUDED.low,\n",
    "            close = EXCLUDED.close,\n",
    "            volume = EXCLUDED.volume,\n",
    "            sentiment_mean = EXCLUDED.sentiment_mean,\n",
    "            sentiment_min = EXCLUDED.sentiment_min,\n",
    "            sentiment_max = EXCLUDED.sentiment_max,\n",
    "            sentiment_count = EXCLUDED.sentiment_count;\n",
    "    \"\"\", (\n",
    "        row['Stock'], row['Date'], row['Open'], row['High'], row['Low'],\n",
    "        row['Close'], row['Volume'],\n",
    "        row['sentiment_mean'], row['sentiment_min'],\n",
    "        row['sentiment_max'], row['sentiment_count']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"âœ… Real-time stock + sentiment row stored for {yesterday_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess, Sentiment analysis of 60 days 5 min data (MY Turn NOw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# --- AWS & DB CONFIGURATION ---\n",
    "S3_BUCKET = \"data608-2025-stock-market-data\"\n",
    "STOCK_KEY = \"real-time-stock-data/stock_60d_5m.csv\"\n",
    "NEWS_KEY = \"real-time-news-data/historical_news_1yr.csv\"\n",
    "DB_NAME = \"your_db_name\"\n",
    "DB_USER = \"your_username\"\n",
    "DB_PASSWORD = \"your_password\"\n",
    "DB_HOST = \"your-db-host-name.ca-central-1.rds.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "TABLE_NAME = \"realtime_60day_merged\"\n",
    "\n",
    "# --- Load from S3 ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "stock_obj = s3.get_object(Bucket=S3_BUCKET, Key=STOCK_KEY)\n",
    "news_obj = s3.get_object(Bucket=S3_BUCKET, Key=NEWS_KEY)\n",
    "\n",
    "stock_df = pd.read_csv(stock_obj[\"Body\"])\n",
    "news_df = pd.read_csv(news_obj[\"Body\"])\n",
    "\n",
    "# --- Preprocessing Step 1: Datetime Alignment ---\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"], utc=True)\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "news_df[\"publishedDate\"] = pd.to_datetime(news_df[\"publishedDate\"], errors=\"coerce\")\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.floor(\"min\")\n",
    "news_df[\"publishedDate\"] = news_df[\"publishedDate\"].dt.floor(\"min\")\n",
    "\n",
    "# --- Filter news for same date range as stock ---\n",
    "min_date = stock_df[\"Date\"].min().date()\n",
    "max_date = stock_df[\"Date\"].max().date()\n",
    "filtered_news = news_df[\n",
    "    (news_df[\"publishedDate\"].dt.date >= min_date) &\n",
    "    (news_df[\"publishedDate\"].dt.date <= max_date)\n",
    "].copy()\n",
    "filtered_news.rename(columns={\"symbol\": \"Stock\"}, inplace=True)\n",
    "\n",
    "# --- Assign Latest News to Each Stock Row ---\n",
    "stock_df.sort_values([\"Stock\", \"Date\"], inplace=True)\n",
    "filtered_news.sort_values([\"Stock\", \"publishedDate\"], inplace=True)\n",
    "\n",
    "assigned_news = []\n",
    "for symbol, group in stock_df.groupby(\"Stock\"):\n",
    "    stock_times = group[\"Date\"].values\n",
    "    news_times = filtered_news[filtered_news[\"Stock\"] == symbol][[\"publishedDate\", \"text\"]].values\n",
    "    latest_news = None\n",
    "    news_index = 0\n",
    "    for stock_time in stock_times:\n",
    "        while news_index < len(news_times) and news_times[news_index][0] <= stock_time:\n",
    "            latest_news = news_times[news_index][1]\n",
    "            news_index += 1\n",
    "        assigned_news.append(latest_news)\n",
    "\n",
    "stock_df[\"latest_news\"] = assigned_news\n",
    "\n",
    "# --- Sentiment Analysis using FinBERT ---\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def get_finbert_score(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            scores = probs[0].tolist()\n",
    "            return -1 * scores[0] + 0 * scores[1] + 1 * scores[2]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "unique_news = stock_df[\"latest_news\"].dropna().unique()\n",
    "news_sentiment_map = {news: get_finbert_score(news) for news in tqdm(unique_news)}\n",
    "\n",
    "# --- Sentiment Fading ---\n",
    "faded_scores = []\n",
    "fade_factor = 0.9\n",
    "prev_news = None\n",
    "prev_score = 0.0\n",
    "repeat_count = 0\n",
    "\n",
    "for news in stock_df[\"latest_news\"]:\n",
    "    if news != prev_news:\n",
    "        sentiment = news_sentiment_map.get(news, 0.0)\n",
    "        repeat_count = 0\n",
    "    else:\n",
    "        repeat_count += 1\n",
    "        sentiment = prev_score * (fade_factor ** repeat_count)\n",
    "    faded_scores.append(round(sentiment, 4))\n",
    "    prev_news = news\n",
    "    prev_score = news_sentiment_map.get(news, 0.0)\n",
    "\n",
    "stock_df[\"faded_sentiment_score\"] = faded_scores\n",
    "\n",
    "# --- Store Final DataFrame to RDS PostgreSQL ---\n",
    "print(\"Uploading final dataset to RDS PostgreSQL...\")\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# You can change if_exists to \"replace\" or \"append\" based on your use case\n",
    "stock_df.to_sql(TABLE_NAME, engine, index=False, if_exists=\"replace\")\n",
    "print(f\"âœ… Table '{TABLE_NAME}' uploaded to RDS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time stock data and news collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ”§ Config\n",
    "API_KEY = \"hDdU807PlusyraTBnNgkkm2gFuPtkZ9F\"\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "now = datetime.now()\n",
    "lookback_days = 4\n",
    "start_date = now - timedelta(days=lookback_days)\n",
    "\n",
    "# RDS Connection (Update these)\n",
    "DB_USER = \"your_username\"\n",
    "DB_PASSWORD = \"your_password\"\n",
    "DB_HOST = \"your-db-hostname.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"your_database\"\n",
    "TABLE_NAME = \"latest_stock_sentiment\"\n",
    "conn_str = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“ˆ Step 1: Fetch Latest Stock Prices\n",
    "stock_data = []\n",
    "for ticker in tickers:\n",
    "    df = yf.Ticker(ticker).history(period=\"1d\", interval=\"5m\")\n",
    "    if not df.empty:\n",
    "        latest_row = df.iloc[-1].copy()\n",
    "        latest_row[\"Stock\"] = ticker\n",
    "        latest_row[\"Datetime\"] = latest_row.name\n",
    "        stock_data.append(latest_row)\n",
    "\n",
    "stock_df = pd.DataFrame(stock_data)\n",
    "stock_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert to ET and make naive\n",
    "stock_df[\"Datetime\"] = pd.to_datetime(stock_df[\"Datetime\"], utc=True)\n",
    "stock_df[\"Datetime\"] = stock_df[\"Datetime\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ“° Step 2: Fetch Latest News\n",
    "def fetch_latest_news(ticker):\n",
    "    url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "    params = {\n",
    "        \"tickers\": ticker,\n",
    "        \"from\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"to\": now.strftime(\"%Y-%m-%d\"),\n",
    "        \"limit\": 50,\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        if isinstance(data, list) and data:\n",
    "            return data[0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "stock_df[\"latest_news\"] = stock_df[\"Stock\"].apply(fetch_latest_news)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ¤– Step 3: Sentiment Analysis\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def get_weighted_sentiment(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            scores = probs[0].tolist()\n",
    "            return round(-1 * scores[0] + 0 * scores[1] + 1 * scores[2], 4)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "stock_df[\"sentiment_score\"] = stock_df[\"latest_news\"].apply(\n",
    "    lambda x: get_weighted_sentiment(str(x)) if pd.notna(x) else 0.0\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ðŸ’¾ Step 4: Save to PostgreSQL Table\n",
    "try:\n",
    "    stock_df.to_sql(TABLE_NAME, engine, if_exists=\"append\", index=False)\n",
    "    print(f\"âœ… Data saved to RDS table: {TABLE_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving to database: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving the values from the table that contains only 1 day data to the database that contain the 60 days data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Database connection details\n",
    "DB_USER = \"your_username\"\n",
    "DB_PASSWORD = \"your_password\"\n",
    "DB_HOST = \"your-db-host.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"your_database\"\n",
    "SOURCE_TABLE = \"latest_stock_sentiment\"\n",
    "TARGET_TABLE = \"stock_sentiment_60d\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Setup DB connection\n",
    "conn_str = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Get yesterday's date\n",
    "yesterday = (datetime.now() - timedelta(days=1)).date()\n",
    "\n",
    "# Step 1: Read yesterday's real-time data\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM {SOURCE_TABLE}\n",
    "    WHERE DATE(\"Datetime\") = '{yesterday}'\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n",
    "print(f\"âœ… Fetched {len(df)} rows from {SOURCE_TABLE} for {yesterday}\")\n",
    "\n",
    "# Step 2: Append to 60-day table\n",
    "if not df.empty:\n",
    "    df.to_sql(TARGET_TABLE, engine, if_exists=\"append\", index=False)\n",
    "    print(f\"âœ… Appended data to {TARGET_TABLE}\")\n",
    "\n",
    "# Step 3: Optional - Remove rows older than 60 days\n",
    "delete_query = f\"\"\"\n",
    "    DELETE FROM {TARGET_TABLE}\n",
    "    WHERE \"Datetime\" < NOW() - INTERVAL '60 days'\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(delete_query)\n",
    "    print(\"ðŸ§¹ Removed rows older than 60 days from target table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to load the 1 year stock data from the s3 bucket, clean the date column and then store it to a database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load 1-Year Stock Data â†’ Clean â†’ Upload to PostgreSQL\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# --- AWS S3 Configuration ---\n",
    "bucket = \"data608-2025-stock-market-data\"\n",
    "stock_key = \"historical-stock-data/stock_1y_1d.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# --- Load stock data from S3 ---\n",
    "response = s3.get_object(Bucket=bucket, Key=stock_key)\n",
    "df_stock = pd.read_csv(response[\"Body\"])\n",
    "\n",
    "# --- Clean the Date column ---\n",
    "df_stock[\"Date\"] = pd.to_datetime(df_stock[\"Date\"], utc=True, errors=\"coerce\")\n",
    "df_stock[\"Date\"] = df_stock[\"Date\"].dt.tz_convert(None)  # Make timezone-naive\n",
    "df_stock = df_stock.dropna(subset=[\"Date\"])  # Drop any invalid rows\n",
    "\n",
    "# Optional: Sort by stock and date\n",
    "df_stock = df_stock.sort_values([\"Stock\", \"Date\"])\n",
    "\n",
    "# --- PostgreSQL RDS Configuration ---\n",
    "rds_host = \"your-rds-endpoint.rds.amazonaws.com\"\n",
    "rds_port = \"5432\"\n",
    "rds_db = \"postgres\"\n",
    "rds_user = \"admin\"\n",
    "rds_password = \"your_password\"\n",
    "table_name = \"historical_stock_1y_daily\"\n",
    "\n",
    "# --- Upload to PostgreSQL ---\n",
    "engine = create_engine(f'postgresql+psycopg2://{rds_user}:{rds_password}@{rds_host}:{rds_port}/{rds_db}')\n",
    "df_stock.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"âœ… Uploaded 1-year stock data to RDS table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to get the real time daily stock data (We will run this every morning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Add New Daily Stock Data to 1-Year Table (No Sentiment)\n",
    "# ===========================\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Config\n",
    "stocks = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "RDS_HOST = 'your-db-endpoint.rds.amazonaws.com'\n",
    "RDS_DB = 'your_db'\n",
    "RDS_USER = 'your_user'\n",
    "RDS_PASS = 'your_password'\n",
    "TABLE_NAME = \"historical_stock_1y_daily\"\n",
    "\n",
    "# Get yesterday's date\n",
    "yesterday = datetime.utcnow().date() - timedelta(days=1)\n",
    "\n",
    "# Fetch yesterday's stock data\n",
    "stock_rows = []\n",
    "for symbol in stocks:\n",
    "    data = yf.Ticker(symbol).history(period=\"2d\", interval=\"1d\").reset_index()\n",
    "    if len(data) > 1:\n",
    "        row = data.iloc[-1]\n",
    "        stock_rows.append({\n",
    "            \"Stock\": symbol,\n",
    "            \"Date\": row[\"Date\"].date(),\n",
    "            \"Open\": row[\"Open\"],\n",
    "            \"High\": row[\"High\"],\n",
    "            \"Low\": row[\"Low\"],\n",
    "            \"Close\": row[\"Close\"],\n",
    "            \"Volume\": row[\"Volume\"]\n",
    "        })\n",
    "\n",
    "# Insert into RDS\n",
    "df = pd.DataFrame(stock_rows)\n",
    "conn = psycopg2.connect(\n",
    "    host=RDS_HOST,\n",
    "    database=RDS_DB,\n",
    "    user=RDS_USER,\n",
    "    password=RDS_PASS\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO {TABLE_NAME} (\n",
    "            stock, date, open, high, low, close, volume\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (stock, date) DO NOTHING;\n",
    "    \"\"\", (\n",
    "        row['Stock'], row['Date'], row['Open'], row['High'],\n",
    "        row['Low'], row['Close'], row['Volume']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"âœ… Daily stock prices for {yesterday} added to {TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to train the model every morning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Train SARIMAX Models for Each Stock (60-Day, 5-Min Data)\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sqlalchemy import create_engine\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- DB CONFIGURATION ---\n",
    "DB_USER = \"your_username\"\n",
    "DB_PASSWORD = \"your_password\"\n",
    "DB_HOST = \"your-db-host-name.ca-central-1.rds.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"your_db_name\"\n",
    "TABLE_NAME = \"realtime_60day_merged\"\n",
    "\n",
    "# --- Local Folder to Save Models ---\n",
    "MODEL_SAVE_PATH = \"./trained_models/\"  # Or 's3://your-bucket/models/' for S3 version\n",
    "\n",
    "# --- Best Model Orders (from evaluation) ---\n",
    "best_model_orders = {\n",
    "    \"AAPL\": (3, 1, 4),\n",
    "    \"META\": (2, 1, 2),\n",
    "    \"NVDA\": (0, 1, 0),\n",
    "    \"GOOGL\": (1, 1, 0),\n",
    "    \"TSLA\": (0, 1, 0),\n",
    "    \"AMZN\": (1, 1, 0),\n",
    "    \"MSFT\": (1, 1, 1)\n",
    "}\n",
    "\n",
    "# --- Connect to PostgreSQL and Load Data ---\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "df = pd.read_sql(f\"SELECT * FROM {TABLE_NAME}\", engine)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df['EMA_5'] = df['Close'].ewm(span=5, adjust=False).mean()\n",
    "    df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['Volatility_30min'] = df['Close'].rolling(window=6).std()\n",
    "    df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "    df['Lag_Close'] = df['Close'].shift(1)\n",
    "    df['Weekday'] = df['Date'].dt.weekday\n",
    "    return df\n",
    "\n",
    "# --- Train and Save Models ---\n",
    "for stock in best_model_orders:\n",
    "    print(f\"\\nðŸ“¦ Training model for {stock}\")\n",
    "    stock_df = df[df[\"Stock\"] == stock].copy()\n",
    "    stock_df = add_features(stock_df)\n",
    "    stock_df.dropna(inplace=True)\n",
    "    stock_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "    features = ['Volume', 'faded_sentiment_score', 'EMA_5', 'EMA_12',\n",
    "                'Volatility_30min', 'Price_Change_Pct', 'Lag_Close', 'Weekday']\n",
    "\n",
    "    if len(stock_df) < 100:\n",
    "        print(f\"âŒ Not enough data for {stock}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    y = stock_df['Close']\n",
    "    X = stock_df[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    order = best_model_orders[stock]\n",
    "\n",
    "    try:\n",
    "        model = SARIMAX(y, exog=X_scaled, order=order, enforce_stationarity=False)\n",
    "        result = model.fit(disp=False)\n",
    "        \n",
    "        # Save model & scaler locally\n",
    "        joblib.dump(result, f\"{MODEL_SAVE_PATH}{stock}_sarimax.pkl\")\n",
    "        joblib.dump(scaler, f\"{MODEL_SAVE_PATH}{stock}_scaler.pkl\")\n",
    "        print(f\"âœ… Saved model and scaler for {stock}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error training model for {stock}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-Time Data Fetch + Sentiment + Fading + Append to RDS Table + adding the feature engineering to the latest row + making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "API_KEY = \"your_fmp_api_key\"\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "now = datetime.now()\n",
    "fade_factor = 0.9\n",
    "\n",
    "# DB Config\n",
    "DB_USER = \"your_user\"\n",
    "DB_PASSWORD = \"your_password\"\n",
    "DB_HOST = \"your-db-host.ca-central-1.rds.amazonaws.com\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"your_db\"\n",
    "DATA_TABLE = \"realtime_60day_merged\"\n",
    "PRED_TABLE = \"realtime_60min_predictions\"\n",
    "\n",
    "# Model directory (each model is saved as {stock}.pkl)\n",
    "MODEL_DIR = \"models\"\n",
    "\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1: Fetch Real-Time Stock â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "stock_data = []\n",
    "for ticker in tickers:\n",
    "    df = yf.Ticker(ticker).history(period=\"1d\", interval=\"5m\")\n",
    "    if not df.empty:\n",
    "        latest_row = df.iloc[-1].copy()\n",
    "        latest_row[\"Stock\"] = ticker\n",
    "        latest_row[\"Date\"] = latest_row.name\n",
    "        stock_data.append(latest_row)\n",
    "\n",
    "stock_df = pd.DataFrame(stock_data)\n",
    "stock_df.reset_index(drop=True, inplace=True)\n",
    "stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"], utc=True)\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.tz_convert(\"America/New_York\").dt.tz_localize(None)\n",
    "stock_df[\"Date\"] = stock_df[\"Date\"].dt.floor(\"min\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2: Fetch Latest News â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_latest_news(ticker):\n",
    "    url = \"https://financialmodelingprep.com/api/v3/stock_news\"\n",
    "    params = {\n",
    "        \"tickers\": ticker,\n",
    "        \"from\": (now - timedelta(days=2)).strftime(\"%Y-%m-%d\"),\n",
    "        \"to\": now.strftime(\"%Y-%m-%d\"),\n",
    "        \"limit\": 50,\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            return data[0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "stock_df[\"latest_news\"] = stock_df[\"Stock\"].apply(fetch_latest_news)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3: Sentiment Analysis (FinBERT) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = softmax(logits, dim=1)[0]\n",
    "            score = -1 * probs[0].item() + 1 * probs[2].item()\n",
    "            return round(score, 4)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "unique_news = stock_df[\"latest_news\"].dropna().unique()\n",
    "news_sentiment_map = {news: get_sentiment_score(news) for news in tqdm(unique_news)}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4: Sentiment Fading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "faded_scores = []\n",
    "prev_news = None\n",
    "prev_score = 0.0\n",
    "repeat_count = 0\n",
    "\n",
    "for news in stock_df[\"latest_news\"]:\n",
    "    if news != prev_news:\n",
    "        sentiment = news_sentiment_map.get(news, 0.0)\n",
    "        repeat_count = 0\n",
    "    else:\n",
    "        repeat_count += 1\n",
    "        sentiment = prev_score * (fade_factor ** repeat_count)\n",
    "    faded_scores.append(round(sentiment, 4))\n",
    "    prev_news = news\n",
    "    prev_score = news_sentiment_map.get(news, 0.0)\n",
    "\n",
    "stock_df[\"faded_sentiment_score\"] = faded_scores\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 5: Append Real-Time Row to Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "stock_df.to_sql(DATA_TABLE, engine, if_exists=\"append\", index=False)\n",
    "print(f\"âœ… Real-time row appended to {DATA_TABLE}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 6: Forecast Using Last 50 Rows + Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_intraday_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"EMA_5\"] = df[\"Close\"].ewm(span=5, adjust=False).mean()\n",
    "    df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    df[\"Volatility_30min\"] = df[\"Close\"].rolling(window=6).std()\n",
    "    df[\"Price_Change_Pct\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Lag_Close\"] = df[\"Close\"].shift(1)\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    return df\n",
    "\n",
    "forecast_steps = {\n",
    "    \"15min\": 3,\n",
    "    \"30min\": 6,\n",
    "    \"1hour\": 12\n",
    "}\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for stock in tickers:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {DATA_TABLE} WHERE \\\"Stock\\\" = '{stock}' ORDER BY \\\"Date\\\" DESC LIMIT 60\", engine)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df = df.sort_values(\"Date\")\n",
    "    df = add_intraday_features(df)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Volume\", \"faded_sentiment_score\",\n",
    "        \"EMA_5\", \"EMA_12\", \"Volatility_30min\",\n",
    "        \"Price_Change_Pct\", \"Lag_Close\", \"Weekday\"\n",
    "    ]\n",
    "\n",
    "    df = df[[\"Date\", \"Close\"] + feature_cols].dropna()\n",
    "    if len(df) < 20:\n",
    "        print(f\"âš ï¸ Not enough data for {stock} to predict.\")\n",
    "        continue\n",
    "\n",
    "    endog = df[\"Close\"]\n",
    "    exog = df[feature_cols]\n",
    "    exog_scaled = pd.DataFrame(StandardScaler().fit_transform(exog), columns=exog.columns, index=exog.index)\n",
    "\n",
    "    try:\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{stock}.pkl\")\n",
    "        saved_model = joblib.load(model_path)\n",
    "\n",
    "        for label, steps in forecast_steps.items():\n",
    "            forecast = saved_model.forecast(steps=steps, exog=exog_scaled.iloc[-steps:])\n",
    "            forecast_mean = forecast.mean()\n",
    "            current_price = endog.iloc[-1]\n",
    "            direction = \"Up\" if forecast_mean > current_price else \"Down\"\n",
    "\n",
    "            all_preds.append({\n",
    "                \"Stock\": stock,\n",
    "                \"Forecast_Horizon\": label,\n",
    "                \"Timestamp\": endog.index[-1],\n",
    "                \"Current_Price\": round(current_price, 4),\n",
    "                \"Forecast_Avg\": round(forecast_mean, 4),\n",
    "                \"Predicted_Trend\": direction\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Prediction failed for {stock}: {e}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 7: Save Predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if all_preds:\n",
    "    pred_df = pd.DataFrame(all_preds)\n",
    "    pred_df.to_sql(PRED_TABLE, engine, if_exists=\"append\", index=False)\n",
    "    print(f\"âœ… Predictions saved to {PRED_TABLE}\")\n",
    "else:\n",
    "    print(\"âŒ No predictions made.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
